{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827af688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numdifftools as ndt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from pyDOE import *\n",
    "from scipy.linalg import cholesky\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy import signal\n",
    "from eofs.standard import Eof\n",
    "from netCDF4 import Dataset\n",
    "from scipy.interpolate import griddata\n",
    "from mySSA import mySSA\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa import stattools\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.integrate import nquad, quad\n",
    "\n",
    "from tsmoothie.utils_func import sim_seasonal_data\n",
    "from tsmoothie.smoother import ConvolutionSmoother\n",
    "from tsmoothie.bootstrap import BootstrappingWrapper\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8083478",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3983df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ukf_ndim(para):\n",
    "     \n",
    "    \"\"\"\n",
    "    Performs ukf (na-dimentions)\n",
    "\n",
    "    Parameters:\n",
    "    - para (list): parameters for the potential\n",
    "\n",
    "    Returns:\n",
    "    float: log likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data)\n",
    "    na = 1\n",
    "    na2 = na*2\n",
    "    H = np.ones((1, na))\n",
    "    h = 0.001\n",
    "    R = para[4]**2\n",
    "    Q = h*para[5]**2\n",
    "    transient = 50\n",
    "    x_pre = [data[0]]\n",
    "    p_pre = np.full((na, na), R)\n",
    "    loc_lnlike = np.zeros(n)\n",
    "    \n",
    "    for k in range(2,n+1):\n",
    "        \n",
    "        for l in range(1,L+1):\n",
    "            \n",
    "            A = np.linalg.cholesky(na*p_pre)\n",
    "            \n",
    "            sig_p = np.hstack((x_pre + A, x_pre - A))\n",
    "            \n",
    "            if ukf_ndim.mode == \"full\":\n",
    "                sig_p[0, :] = sig_p[0, :] - h * (4.0 * para[3] * sig_p[0, :] ** 3 + 3.0 \n",
    "                                             * para[2] * sig_p[0, :] ** 2 + 2.0 * para[1] * sig_p[0, :] + para[0]\n",
    "                                                + para[6] * benthic[k-1] + para[7] * sol[k-1])\n",
    "            if ukf_ndim.mode == \"no\":\n",
    "                sig_p[0, :] = sig_p[0, :] - h * (4.0 * para[3] * sig_p[0, :] ** 3 + 3.0 \n",
    "                                             * para[2] * sig_p[0, :] ** 2 + 2.0 * para[1] * sig_p[0, :] + para[0])\n",
    "                \n",
    "            if ukf_ndim.mode == \"benthic\":\n",
    "                sig_p[0, :] = sig_p[0, :] - h * (4.0 * para[3] * sig_p[0, :] ** 3 + 3.0 \n",
    "                                             * para[2] * sig_p[0, :] ** 2 + 2.0 * para[1] * sig_p[0, :] + para[0]\n",
    "                                                + para[6] * benthic[k-1])\n",
    "            if ukf_ndim.mode == \"sol\":\n",
    "                sig_p[0, :] = sig_p[0, :] - h * (4.0 * para[3] * sig_p[0, :] ** 3 + 3.0 \n",
    "                                             * para[2] * sig_p[0, :] ** 2 + 2.0 * para[1] * sig_p[0, :] + para[0]\n",
    "                                                + para[6] * sol[k-1])\n",
    "            \n",
    "            x_pre = np.mean(sig_p, axis=1)\n",
    "            p_pre = np.zeros((na, na))\n",
    "        \n",
    "            for i in range(1,na2+1):\n",
    "                p_pre += np.outer(sig_p[:, i-1] - x_pre, sig_p[:, i-1] - x_pre)\n",
    "                \n",
    "            p_pre = p_pre/na2+Q\n",
    "            \n",
    "        S = H @ p_pre @ H.T + R   \n",
    "        K = p_pre @ H.T @ np.linalg.inv(S)\n",
    "        resi = data[k-1] - H @ x_pre    \n",
    "        loc_lnlike[k-1] = -(np.log(np.linalg.det(S)) + np.dot(resi.T, np.linalg.solve(S, resi))) / 2 \n",
    "        x_pre = x_pre + K @ resi        \n",
    "        p_pre = np.dot((np.eye(na) - np.dot(K, H)), p_pre) \n",
    "    \n",
    "    return - (-(n-transient)/2*np.log(2*np.pi)+sum(loc_lnlike[(transient):n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a5e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def ukf_1dim(para, h):\n",
    "def ukf_1dim(para):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs ukf (1-dimension)\n",
    "\n",
    "    Parameters:\n",
    "    - para (list): parameters for the potential\n",
    "    - h (float): stepsize of ukf\n",
    "\n",
    "    Returns:\n",
    "    float: log likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(data)\n",
    "    h = 0.001   \n",
    "    R = para[4]**2 \n",
    "    Q = h*para[5]**2 \n",
    "    transient = 50 \n",
    "    L = int(dT/h)\n",
    "\n",
    "    x_pre = np.zeros(n)\n",
    "    p_pre = np.zeros(n)\n",
    "    \n",
    "    x_pre[0] = data[0]\n",
    "    \n",
    "    if p_pre[0] == 0:\n",
    "        p_pre[0] = R\n",
    "    \n",
    "    loc_lnlike = np.zeros(n)\n",
    "    \n",
    "    for k in range(1,n):\n",
    "        \n",
    "        x_pre[k] = x_pre[k-1]\n",
    "        p_pre[k] = p_pre[k-1]\n",
    "        \n",
    "        for l in range(1, L+1): \n",
    "            A = np.sqrt(p_pre[k])\n",
    "            sp = [x_pre[k]+A,x_pre[k]-A]\n",
    "            \n",
    "            if ukf_1dim.mode == \"full\":\n",
    "                if ukf_1dim.hem==\"north\":\n",
    "                    sp = [x - h*(4*para[3]*x**3 + 3*para[2]*x**2 + 2*para[1]*x + para[0] + para[6] * benthic[k] + para[7] * soln[k]) for x in sp]\n",
    "                if ukf_1dim.hem==\"south\":\n",
    "                    sp = [x - h*(4*para[3]*x**3 + 3*para[2]*x**2 + 2*para[1]*x + para[0] + para[6] * benthic[k] + para[7] * sols[k]) for x in sp]\n",
    "            if ukf_1dim.mode == \"no\":\n",
    "                sp = [x - h*(4*para[3]*x**3 + 3*para[2]*x**2 + 2*para[1]*x + para[0]) for x in sp]\n",
    "            if ukf_1dim.mode == \"benthic\":\n",
    "                sp = [x - h*(4*para[3]*x**3 + 3*para[2]*x**2 + 2*para[1]*x + para[0] + para[6] * benthic[k]) for x in sp]\n",
    "            if ukf_1dim.mode == \"sol\":\n",
    "                if ukf_1dim.hem==\"north\":\n",
    "                    sp = [x - h*(4*para[3]*x**3 + 3*para[2]*x**2 + 2*para[1]*x + para[0] + para[6] * soln[k]) for x in sp]\n",
    "                if ukf_1dim.hem==\"south\":\n",
    "                    sp = [x - h*(4*para[3]*x**3 + 3*para[2]*x**2 + 2*para[1]*x + para[0] + para[6] * sols[k]) for x in sp]\n",
    "                    \n",
    "            x_pre[k] = np.mean(sp)\n",
    "            p_pre[k] = np.var(sp) + Q\n",
    "           \n",
    "        S = p_pre[k]+R\n",
    "        K = p_pre[k]/S\n",
    "        resi = data[k]-x_pre[k]\n",
    "        loc_lnlike[k] = -(np.log(S)+resi**2/S)/2\n",
    "        x_pre[k] = x_pre[k] + K * resi\n",
    "        p_pre[k] = (1-K)*p_pre[k]\n",
    "               \n",
    "    r = - (-(n-transient)/2*np.log(2*np.pi)+sum(loc_lnlike[(transient):n]))\n",
    "    if r is None or isinstance(r, float) and math.isnan(r):\n",
    "        return -100000000\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8aff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cut(data, start, end):\n",
    "    \n",
    "    \"\"\"\n",
    "    cut data to a specific time period. \n",
    "\n",
    "    Parameters:\n",
    "    - data (list): data that wants to be shorten in time\n",
    "    - start (int): start date, must be larger than minimum time of data\n",
    "    - end (int):   end date, must be shorten than maximimum time of data\n",
    "\n",
    "    Returns:\n",
    "    list: [cut time, cut data]\n",
    "    \"\"\"\n",
    "    \n",
    "    start_index = next((i for i, value in enumerate(t) if value >= start), None)\n",
    "    end_index = next((i for i, value in reversed(list(enumerate(t))) if value <= end), None)\n",
    "\n",
    "    if start_index is not None and end_index is not None:\n",
    "        t_ = t[start_index:end_index + 1]\n",
    "        data_ = data[start_index:end_index + 1]\n",
    "        return t_, data_\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be156b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSA(data, K, suspected_seasonality, start, end):\n",
    "    \"\"\"\n",
    "    performs singular component analysis between data1 and data2\n",
    "    \n",
    "    Parameters:\n",
    "    - data1 (list): time series data1\n",
    "    - data2 (list): time series data2\n",
    "    \n",
    "    Returns:\n",
    "    [0]: \n",
    "    [1]: \n",
    "    \"\"\"\n",
    "    \n",
    "    ts = pd.DataFrame({'y': data})\n",
    "    \n",
    "    start_year = 2000\n",
    "    num_rows = len(data) \n",
    "\n",
    "    date_index = pd.date_range(start=f\"{start_year}-01-01\", periods=num_rows, freq='H')\n",
    "\n",
    "    ts.index = date_index\n",
    "    ssa = mySSA(ts)\n",
    "\n",
    "    #K = int(10 / dT)\n",
    "    K = K\n",
    "    #suspected_seasonality = 5\n",
    "    suspected_seasonality = suspected_seasonality\n",
    "    \n",
    "    #Embed the time series by forming a Hankel matrix of lagged window (length K) vectors\n",
    "    ssa.embed(embedding_dimension=K, suspected_frequency=suspected_seasonality, verbose=False)\n",
    "    #Decompose the embedded time series via Singular Value Decomposition\n",
    "    ssa.decompose(verbose=False)\n",
    "    \n",
    "    streams = [i for i in range(start,end)]\n",
    "    reconstructed = ssa.view_reconstruction(*[ssa.Xs[i] for i in streams], \n",
    "                                          names=streams, return_df=True, plot=False)\n",
    "\n",
    "\n",
    "    return reconstructed[\"Reconstruction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential(x):\n",
    "    \"\"\"\n",
    "    calculates the potential for given parameters\n",
    "    \n",
    "    Parameters:\n",
    "    - x (list): parameters of potential\n",
    "    \n",
    "    Returns:\n",
    "    list: \n",
    "    \"\"\"\n",
    "    return (p[3]*x**4 + p[2]*x**3 + p[1]*x**2 + p[0]*x)\n",
    "\n",
    "def potential_fit(x):\n",
    "    \"\"\"\n",
    "    performs singular component analysis between data1 and data2\n",
    "    \n",
    "    Parameters:\n",
    "    - data1 (list): time series data1\n",
    "    - data2 (list): time series data2\n",
    "    \n",
    "    Returns:\n",
    "    [0]: \n",
    "    [1]: \n",
    "    \"\"\"\n",
    "    return (fit.x[3]*x**4 + fit.x[2]*x**3 + fit.x[1]*x**2 + fit.x[0]*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc48d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_potential(x, p):\n",
    "    return (p[3]*x**4 + p[2]*x**3 + p[1]*x**2 + p[0]*x)\n",
    "\n",
    "def plot_error(param, hess, label):\n",
    "    #line = plt.plot(xx, [plot_potential(x, param) for x in xx], label = str(label))\n",
    "    param_samples = np.random.multivariate_normal(param, np.diag(hess), num_samples)\n",
    "    potential_samples = np.array([plot_potential(xx, p) for p in param_samples])\n",
    "    for i in range(len(potential_samples)):\n",
    "        plt.plot(xx, potential_samples[i], c=\"k\", alpha=0.02)\n",
    "    #quantiles = np.percentile(potential_samples, [40, 100-40], axis=0)\n",
    "    #plt.fill_between(xx[:,0], quantiles[0][:,0], quantiles[1][:,0], alpha=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a203287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_std(dx, num):\n",
    "    curve = [potential_fit(x) for x in xx]\n",
    "    for i in range(num):\n",
    "        a = num\n",
    "        dcurve = [np.sqrt((x**4*i/a*dx[3])**2+(x**3*i/a*dx[2])**2+(x**2*i/a*dx[1])**2+(x*i/a*dx[0])**2) for x in xx]\n",
    "        plt.fill_between(xx, [a-b for a, b in zip(curve, dcurve)], [a+b for a, b in zip(curve, dcurve)],color = \"k\", alpha = 1/a, edgecolor = \"none\")\n",
    "    \n",
    "def plot_std_(dx, color=\"k\"):\n",
    "    curve = [potential_fit(x) for x in xx]\n",
    "    dcurve = [np.sqrt((x**4*dx[3])**2+(x**3*dx[2])**2+(x**2*dx[1])**2+(x*dx[0])**2) for x in xx]\n",
    "    plt.fill_between(xx, [a-b for a, b in zip(curve, dcurve)], [a+b for a, b in zip(curve, dcurve)],color = color, alpha = 0.6, edgecolor = \"none\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05687617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_corr(x,y):\n",
    "\n",
    "    def ccf_values(series1, series2):\n",
    "        p = series1\n",
    "        q = series2\n",
    "        p = (p - np.mean(p)) / (np.std(p) * len(p))\n",
    "        q = (q - np.mean(q)) / (np.std(q))  \n",
    "        c = np.correlate(p, q, 'full')\n",
    "        return c\n",
    "\n",
    "    ccf_ielts = ccf_values(x, y)\n",
    "\n",
    "    lags = signal.correlation_lags(len(x), len(y))\n",
    "\n",
    "    def ccf_plot(lags, ccf):\n",
    "        fig, ax =plt.subplots(figsize=(9, 6))\n",
    "        ax.plot(lags, ccf, color = \"black\")\n",
    "        ax.axhline(-2/np.sqrt(23), color='tab:red', label='5% confidence interval')\n",
    "        ax.axhline(2/np.sqrt(23), color='tab:red')\n",
    "        ax.axvline(x = 0, color = 'black', lw = 1)\n",
    "        ax.axhline(y = 0, color = 'black', lw = 1)\n",
    "        ax.axhline(y = np.max(ccf), color = 'tab:blue', lw = 1, \n",
    "        linestyle='--', label = 'highest +/- correlation')\n",
    "        ax.axhline(y = np.min(ccf), color = 'tab:blue', lw = 1, \n",
    "        linestyle='--')\n",
    "        ax.set(ylim = [-1, 1])\n",
    "        ax.set_ylabel('Correlation Coefficients', fontsize = 12)\n",
    "        ax.set_xlabel('Time Lags', fontsize = 12)\n",
    "        plt.title(\"x leads y          |          y leads x\")\n",
    "        plt.legend()\n",
    "\n",
    "    print(\"lag_max: \", lags[np.argmax(ccf_ielts)])\n",
    "    print(\"lag_min: \", lags[np.argmin(ccf_ielts)])\n",
    "    ccf_plot(lags, ccf_ielts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002247bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_pass(data, order):\n",
    "    \n",
    "    cut_off_frequency = 1.0 / (8/dT) #convert periode of 8000 to frequency to remove milankovic\n",
    "\n",
    "    # Erzeugen des High-Pass-Filters\n",
    "    b, a = butter(order, cut_off_frequency, btype='high', analog=False)\n",
    "\n",
    "    # Anwenden des Filters auf die Zeitreihe\n",
    "    return lfilter(b, a, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e305951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_pass(data, lowcut, highcut, order, dT):\n",
    "    # Konvertiere Perioden zu Frequenzen\n",
    "    lowcut_frequency = 1.0 / (lowcut / dT)\n",
    "    highcut_frequency = 1.0 / (highcut / dT) \n",
    "\n",
    "    # Erzeuge die Koeffizienten für den Bandpass-Filter\n",
    "    b, a = butter(order, [lowcut_frequency, highcut_frequency], btype='band', analog=False)\n",
    "\n",
    "    # Anwenden des Filters auf die Zeitreihe\n",
    "    return lfilter(b, a, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    cumsum = np.cumsum(data, dtype=float)\n",
    "    cumsum[window_size:] = cumsum[window_size:] - cumsum[:-window_size]\n",
    "    return cumsum[window_size - 1:] / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_midpoint(data, window_size):\n",
    "    midpoint_values = []\n",
    "\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        window_data = data[i:i + window_size]\n",
    "\n",
    "        min_value = np.min(window_data)\n",
    "        max_value = np.max(window_data)\n",
    "\n",
    "        midpoint = (min_value + max_value) / 2\n",
    "        midpoint_values.append(midpoint)\n",
    "\n",
    "    return np.array(midpoint_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd1dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_moving_midpoint(data, window_size, window_size_smooth):\n",
    "    laufender_mittelpunkt = moving_midpoint(data, window_size)\n",
    "    smoothed_midpoint = np.convolve(laufender_mittelpunkt, np.ones(window_size_smooth)/window_size_smooth, mode='valid')\n",
    "    \n",
    "    return smoothed_midpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff078ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_moving(data, window_size, mode):\n",
    "    if mode == \"avg\":\n",
    "        moveavg = data[int(window_size/2):-int(window_size/2-1)]-moving_average(data, window_size)\n",
    "        return np.pad(moveavg, (int(window_size/2), int(window_size/2-1)), mode='constant', constant_values=0)\n",
    "\n",
    "    if mode == \"mid\":\n",
    "        movemid = data[int(window_size/2):-int(window_size/2-1)]-moving_midpoint(data, window_size)\n",
    "        return np.pad(movemid, (int(window_size/2), int(window_size/2-1)), mode='constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19796872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def return_band_pass(para):\n",
    "    lowcut = para[0] / (2*np.pi)\n",
    "    highcut = para[1] / (2*np.pi)\n",
    "    sampling_rate = 1 / 0.02  \n",
    "    if lowcut < highcut:\n",
    "        return butter_bandpass_filter(series_forced, lowcut, highcut, sampling_rate, 3)\n",
    "    else:\n",
    "        return np.zeros(len(series_forced))\n",
    "\n",
    "def cost_function(params, y_a):\n",
    "    y_b = return_band_pass(params)\n",
    "    #return np.sum((y_a[bandpass_cut:-bandpass_cut] - y_b[bandpass_cut:-bandpass_cut])**2)\n",
    "    return np.sum((y_a[bandpass_cut:] - y_b[bandpass_cut:])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3fbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasaw(Tn, tau, Ts0):\n",
    "    # Create an interpolation function for Tn\n",
    "    tn_interp = interp1d(np.arange(len(Tn)), Tn, kind='linear')\n",
    "    t_values = np.arange(len(Tn))\n",
    "\n",
    "    # Initialize the time series Ts with zeros\n",
    "    Ts = np.zeros(len(t_values))\n",
    "\n",
    "    # Perform integration for each time step\n",
    "    for i in range(len(t_values)):\n",
    "        t = t_values[i]\n",
    "        integral, _ = quad(lambda t_prime: tn_interp(t - t_prime) * np.exp(-t_prime / tau), 0, t, limit=1000)\n",
    "        Ts[i] = -1 / tau * integral + Ts0 * np.exp(-t / tau)\n",
    "\n",
    "    return Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f61c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biplot(score, coeff , y):\n",
    "    '''\n",
    "    Author: Serafeim Loukas, serafeim.loukas@epfl.ch\n",
    "    Inputs:\n",
    "       score: the projected data\n",
    "       coeff: the eigenvectors (PCs)\n",
    "       y: the class labels\n",
    "   '''    \n",
    "    xs = score[:,0] # projection on PC1\n",
    "    ys = score[:,1] # projection on PC2\n",
    "    n = coeff.shape[0] # number of variables\n",
    "    plt.figure(figsize=(10,8), dpi=100)\n",
    "    classes = np.unique(y)\n",
    "    colors = ['g','r','y']\n",
    "    markers=['o','^','x']\n",
    "    for s,l in enumerate(classes):\n",
    "        plt.scatter(xs[y==l],ys[y==l], c = colors[s], marker=markers[s]) # color based on group\n",
    "    for i in range(n):\n",
    "        #plot as arrows the variable scores (each variable has a score for PC1 and one for PC2)\n",
    "        plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'k', alpha = 0.9,linestyle = '-',linewidth = 1.5, overhang=0.2)\n",
    "        plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, \"Var\"+str(i+1), color = 'k', ha = 'center', va = 'center')\n",
    "\n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))\n",
    "    limx= int(xs.max()) + 1\n",
    "    limy= int(ys.max()) + 1\n",
    "    plt.xlim([-limx,limx])\n",
    "    plt.ylim([-limy,limy])\n",
    "    plt.grid()\n",
    "    plt.tick_params(axis='both', which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73909a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_(data1, data2, comp, i):\n",
    "    combined_data = np.vstack((data1, data2)).T\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(combined_data)\n",
    "    \n",
    "    if i == 1:\n",
    "        # Überprüfen, ob das Vorzeichen der Hauptkomponente geändert werden muss\n",
    "        if np.corrcoef(pca_result[:, comp], data1)[0, 1] < 0:\n",
    "            return -pca_result[:, comp]\n",
    "        else:\n",
    "            return pca_result[:, comp]\n",
    "    else:\n",
    "        return pca_result[:, comp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d1cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_(data1, data2, comp, ref_direction=None):\n",
    "    combined_data = np.vstack((data1, data2)).T\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(combined_data)\n",
    "    \n",
    "    # Check if reference direction is provided and flip PCs if needed\n",
    "    if ref_direction is not None:\n",
    "        if pca.components_[0][0] * ref_direction[0][0] < 0:\n",
    "            pca_result[:, 0] *= -1\n",
    "        if pca.components_[1][1] * ref_direction[1][1] < 0:\n",
    "            pca_result[:, 1] *= -1\n",
    "   \n",
    "    return [pca_result[:, comp], pca.components_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2a17b1",
   "metadata": {},
   "source": [
    "# TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d5117",
   "metadata": {},
   "source": [
    "Funktion: $ax^4 + bx^3 + cx^2 + dx$\n",
    "\n",
    "Parameter input: $a = [d,c,b,a, N_{obs}, N_{dyn}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf9564",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(-108.3, -10.4, dT)\n",
    "t_ = [x for x in t]\n",
    "\n",
    "df = pd.read_table(path+\"insolation_mean_mon_7_65N_100k.txt\", sep = \"     \", header = None, engine = \"python\")\n",
    "df.columns = [\"Time\", \"Insolation\"]\n",
    "df[\"Time\"] = df[\"Time\"]-0.05# + 2.000\n",
    "max_val, min_val = np.round(np.max(df[\"Time\"]),3), np.round(np.min(df[\"Time\"]),3)\n",
    "max_difference = np.round(df['Time'][df[\"Time\"]>t_[0]].diff().abs().max(),3)\n",
    "soln = interp1d(df[\"Time\"], df[\"Insolation\"], kind=\"linear\")(t_)\n",
    "soln_ = soln\n",
    "soln = (soln - np.mean(soln)) / np.std(soln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "dT = 0.02\n",
    "h = 0.001\n",
    "L = int(dT/h)\n",
    "t = np.arange(-97.9, 0, dT)\n",
    "n = len(t) \n",
    "\n",
    "z = 0.7\n",
    "#   * , höhe barriere (negativ) , * ,  steilheit potential (positiv), messrauschen, dynamisches rauschen\n",
    "a = [0.1, -1.6,  0.8, 1, 0.1, 1.5]\n",
    "a = [0.1, -1,  0.8, 1, 0.1, 1.5]\n",
    "#a = [0.1, -1, -0.8, 1, 0.1, 1.5]\n",
    "#a = [0, -1, 0, 1, 0.1, 1.5]\n",
    "\n",
    "### Time integration of the SDE by Euler-Maruyama method ### \n",
    "p = a        \n",
    "\n",
    "forcing = 0.6*soln\n",
    "\n",
    "ys=[]\n",
    "for i in range(4,5):\n",
    "    y = [0] * n  \n",
    "    random.seed(i)                     \n",
    "    y[0] = z + random.gauss(0, p[4])    \n",
    "    for k in range(1, n):\n",
    "        for l in range(int(L)):\n",
    "            z = z - h*(4 * p[3]*z**3 + 3*p[2]*z**2 + 2*p[1]*z + p[0]) + random.gauss(0, p[5] * np.sqrt(h))\n",
    "        y[k] = z + random.gauss(0, p[4]) # observation at every L(=20) years\n",
    "    y += forcing #add periodic forcing\n",
    "    ys.append(y)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "xx = np.arange(-3, 3, 0.005)\n",
    "ax[0].plot(xx, [potential(x) for x in xx], label=\"true\")\n",
    "ax[0].set_ylim(-2,1)\n",
    "ax[0].set_xlim(-2,2)\n",
    "ax[1].plot(t, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfa4b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 2\n",
    "# operate bootstrap\n",
    "np.random.seed(5)\n",
    "\n",
    "bts = BootstrappingWrapper(ConvolutionSmoother(window_len=20, window_type='bartlett'), \n",
    "                           bootstrap_type='mbb', block_length=40)\n",
    "bts_samples = bts.sample(y, n_samples=n_samples)\n",
    "\n",
    "# plot the bootstrapped timeseries\n",
    "plt.figure(figsize=(13,5))\n",
    "plt.plot(t,y)\n",
    "for i in range(1, n_samples):\n",
    "    plt.plot(t, bts_samples.T[:,i] + 3*i)\n",
    "    ys.append(list(bts_samples.T[:,i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fca5fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "suspected_frequency = 0.3\n",
    "bandpass_cut = int(1/dT)\n",
    "\n",
    "series_forced = ys[0]\n",
    "\n",
    "lo = np.array([0.000001, suspected_frequency-0.001])\n",
    "up = np.array([suspected_frequency+0.001, suspected_frequency+1])\n",
    "\n",
    "ensemble = 100\n",
    "set = lhs(len(lo), samples=ensemble)                \n",
    "set = lo + set.dot(np.diag(up - lo))\n",
    "    \n",
    "for i in range(1):\n",
    "    #print(\"Lauf:\",i+1)\n",
    "    fit = minimize(cost_function, set[0], args=(forcing), method=\"L-BFGS-B\", bounds=list(zip(lo, up)))\n",
    "    #print(\"Parameter:\", fit.x, fit.fun)\n",
    "    value = fit.x\n",
    "\n",
    "for i in range(1,ensemble):\n",
    "    #print(\"Lauf:\",i+1)\n",
    "    fit_ = minimize(cost_function, set[i], args=(forcing), method=\"L-BFGS-B\", bounds=list(zip(lo, up)))\n",
    "    #print(\"Parameter:\", fit_.x, fit_.fun)\n",
    "    if np.abs(fit_.fun) < np.abs(fit.fun):\n",
    "        fit = fit_\n",
    "\n",
    "band_pass_optimized = return_band_pass(fit.x)\n",
    "\n",
    "plt.plot(t, forcing, label='true forcing')\n",
    "plt.plot(t, band_pass_optimized, \"--\", c=\"tab:green\", label='band-pass filtered')\n",
    "#plt.plot(t[bandpass_cut:-bandpass_cut], band_pass_optimized[bandpass_cut:-bandpass_cut], c = \"tab:green\", label='optimization part')\n",
    "plt.plot(t[bandpass_cut:], band_pass_optimized[bandpass_cut:], c = \"tab:green\", label='optimization part')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimierte Parameter:\", fit.x, fit.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fe177",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = np.poly1d(np.polyfit(t, soln, 16))\n",
    "plt.plot(t, ngrip-np.mean(ngrip), label='Daten')\n",
    "plt.plot(t, model1(t)-np.mean(model1(t)), label = \"fit\")\n",
    "series_forced = cal\n",
    "plt.plot(t, 3*cal - return_band_pass([0.17646785, 0.37686754]))\n",
    "plt.plot(t, soln, label = \"sol\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373b50f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb_dim = int(2*np.pi/0.25/dT)\n",
    "\n",
    "#y_SSA1 = SSA(y,emb_dim,1,0,1)\n",
    "#y_SSA2 = SSA(y,emb_dim,1,1,2)\n",
    "#y_SSA3 = SSA(y,emb_dim,1,2,3)\n",
    "\n",
    "#df = y_SSA1 + y_SSA2\n",
    "#df.to_csv('y_SSA1und2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83617b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "\n",
    "wavelet = 'db1'  \n",
    "coeffs = pywt.wavedec(y, wavelet)\n",
    "threshold = 1\n",
    "coeffs[1:] = (pywt.threshold(c, threshold, mode=\"hard\") for c in coeffs[1:])\n",
    "denoised_values = pywt.waverec(coeffs, wavelet)\n",
    "y_denoised = denoised_values[:-1]\n",
    "\n",
    "plt.plot(t,y)\n",
    "plt.plot(t,y_denoised)\n",
    "series_forced = y\n",
    "plt.plot(t, return_band_pass([0.02,0.5]))\n",
    "plt.plot(t,forcing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f28ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 516\n",
    "plt.plot(t, forcing)\n",
    "plt.plot(t[int(window_size/2):-int(window_size/2-1)], moving_midpoint(y, window_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b22a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_minimize(window):\n",
    "    return rms_error(y-forcing, subtract_moving(y, window, \"mid\"))\n",
    "\n",
    "def rms_error(x_data, y_data):\n",
    "    return np.sqrt(np.mean((x_data - y_data)**2))\n",
    "\n",
    "window_sizes = np.arange(20,800,2)\n",
    "\n",
    "best_guess = 10\n",
    "\n",
    "for i in window_sizes:\n",
    "    result = to_minimize(i) \n",
    "    if result < to_minimize(best_guess):\n",
    "        best_guess = i\n",
    "        \n",
    "print(best_guess, to_minimize(best_guess))\n",
    "\n",
    "plt.plot(y-forcing)\n",
    "plt.plot(subtract_moving(y, best_guess, \"mid\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_minimize(lag):\n",
    "    return rms_error(y_denoised-forcing+lag, subtract_moving(y, 236, \"mid\"))\n",
    "\n",
    "result = minimize(to_minimize, 5)\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9635599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best moving mid: 516 0.3964177872245981\n",
    "#best moving avg: 546 0.4879361068120511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05adb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = 1\n",
    "all_params = []\n",
    "\n",
    "for i in range(len(ys)):\n",
    "    \n",
    "    data = list(subtract_moving(y, 550, \"mid\"))\n",
    "    data = (data-np.mean(data))/np.std(data)\n",
    "    data = data_cut(data,-60,-25)[1]\n",
    "    \n",
    "    ukf_1dim.mode = \"no\"\n",
    "    ukf_1dim.hem = \"north\"\n",
    "\n",
    "    lo = np.array([-3,-3,-3,0,0.000001,0])\n",
    "    up = np.array([3,3,3,3,3,3])\n",
    "\n",
    "    set = lhs(len(lo), samples=ensemble)                \n",
    "    set = lo + set.dot(np.diag(up - lo))\n",
    "\n",
    "    fit = minimize(ukf_1dim, [1,1,1,1,1,1],  method=\"L-BFGS-B\", bounds=list(zip(lo, up)))\n",
    "    \n",
    "    if all(lo[i] < fit.x[i] < up[i] for i in range(len(fit.x))):\n",
    "        print(\"Added Parameter: \",list(fit.x), \"Fun: \", fit.fun)\n",
    "        all_params.append(fit.x)\n",
    "        plt.plot(xx, [potential_fit(x) for x in xx], label=str(i))\n",
    "        \n",
    "    else:\n",
    "        print(\"NOT added Parameter: \",list(fit.x), \"Fun: \", fit.fun)\n",
    "    \n",
    "    \n",
    "average_params = np.mean(all_params, axis=0)\n",
    "\n",
    "plt.plot(xx, [plot_potential(x,average_params) for x in xx], c = \"k\", label = \"mean\")   \n",
    "plt.plot(xx, [plot_potential(x,a) for x in xx], \"--\", c = \"k\", label=\"true\")\n",
    "\n",
    "plt.ylim(-2,2)\n",
    "plt.xlim(-2,2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = list(fit.x)\n",
    "\n",
    "aic = 2*(len(set[0]) + fit.fun)\n",
    "bic = 2*fit.fun + len(set[0])*np.log(len(data))\n",
    "\n",
    "output.append(fit.fun)\n",
    "output.append(aic)\n",
    "output.append(bic)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05765b5",
   "metadata": {},
   "source": [
    "NOTE: Asymmetrisches Potential: y-forcing & subtract_moving sind verschoben. Das ganze Problem ist bei symmetrischen Potentialen nicht? Wenn vor UKF y-forcing und subract_moving normiert werden, gibt es das Problem nicht mehr. Aber dann kann nicht das richtige Potential mit geplottet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595115e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_std(fit.x)\n",
    "plt.plot(xx, plot_potential(xx, fit.x))\n",
    "plt.ylim(-4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe69fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ys[0], bins = 100, alpha = 0.5, color = \"k\")\n",
    "#plt.hist(ys[1], bins = 100)\n",
    "plt.hist(ys[1], bins = 100, alpha = 0.5, color =\"k\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "nlags = 200\n",
    "autocorr = stattools.acf(ys[0], nlags=nlags)\n",
    "plt.plot(np.linspace(0,5,nlags+1), autocorr, c = \"tab:green\")\n",
    "autocorr = stattools.acf(ys[1], nlags=nlags)\n",
    "plt.plot(np.linspace(0,5,nlags+1), autocorr, c = \"tab:red\")\n",
    "plt.show()\n",
    "\n",
    "crosscorr = signal.correlate(ys[0],ys[1])\n",
    "plt.plot(np.arange(-len(ys[0]) + 1, len(ys[0])), crosscorr, c=\"tab:blue\")\n",
    "crosscorr = signal.correlate(ys[0],ys[1])\n",
    "plt.plot(np.arange(-len(ys[0]) + 1, len(ys[0])), crosscorr, c=\"tab:red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046c8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.x = [ 0.44909434,-1.23368723,  0.67289993,  0.88969982,  0.1174811,   1.43134424]\n",
    "ukf_1dim.mode = \"no\"\n",
    "Hfun = ndt.Hessian(ukf_1dim, full_output=True)\n",
    "hessian_ndt  = Hfun(fit.x)\n",
    "dx = np.sqrt(np.diag(np.linalg.inv(hessian_ndt[0])))\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904e643",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.x = test1\n",
    "plt.plot(xx, plot_potential(xx, fit.x))\n",
    "plot_std(dys0, \"tab:blue\")\n",
    "\n",
    "fit.x = test2\n",
    "plt.plot(xx, plot_potential(xx, fit.x))\n",
    "plot_std(dys1, \"tab:orange\")\n",
    "\n",
    "fit.x = np.mean([test1, test2], axis=0)\n",
    "plt.plot(xx, plot_potential(xx, fit.x))\n",
    "plot_std([0.31481489, 0.20881886, 0.17195082, 0.09419389, 0.0044344 ,0.03474856], \"tab:green\")\n",
    "\n",
    "plt.plot(xx, plot_potential(xx, [0.1, -1.6,  0.8, 1, 0.1, 1.5]))\n",
    "\n",
    "plt.ylim(-2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xx, [potential_fit(x) for x in xx], label=\"simulated\")\n",
    "plt.plot(xx, [plot_potential(x,a) for x in xx], label=\"true\")\n",
    "plot_std(dx, 30)\n",
    "plt.xlim(-3,3)\n",
    "\n",
    "plt.ylim(-2,2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = fit.x\n",
    "w = np.zeros(n)\n",
    "z = y[0]\n",
    "n = len(data)\n",
    "\n",
    "random.seed(2)\n",
    "w[0] = z + random.gauss(0, p[4]) \n",
    "for k in range(1, n):\n",
    "    for l in range(int(L)):\n",
    "        z = z - h*(4 * p[3]*z**3 + 3*p[2]*z**2 + 2*p[1]*z + p[0]) + random.gauss(0, p[5] * np.sqrt(h))\n",
    "    w[k] = z + random.gauss(0, p[4]) # observation at every L(=20) years\n",
    "\n",
    "### plotting two time series\n",
    "plt.plot(t, y, label=\"given\")\n",
    "plt.plot(t, w, label=\"simulated\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef606ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_auto = []\n",
    "for i in range(50):\n",
    "    w[0] = z + random.gauss(0, p[4]) \n",
    "    for k in range(1, n):\n",
    "        for l in range(int(L)):\n",
    "            z = z - h*(4 * p[3]*z**3 + 3*p[2]*z**2 + 2*p[1]*z + p[0]) + random.gauss(0, p[5] * np.sqrt(h))\n",
    "        w[k] = z + random.gauss(0, p[4]) \n",
    "    ensemble_auto.append(stattools.acf(w, nlags=nlags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cdcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y #data to choose\n",
    "nlags = int(5/0.02)\n",
    "\n",
    "autocorr = stattools.acf(x, nlags=nlags)\n",
    "plt.plot(np.linspace(0,5,251), autocorr, c = \"tab:green\")\n",
    "\n",
    "autocorr = stattools.acf(w, nlags=nlags)\n",
    "ensemble_auto_mean = np.mean(ensemble_auto, axis=0)\n",
    "\n",
    "std_deviation = np.std(ensemble_auto, axis=0)\n",
    "plt.plot(np.linspace(0,5,251), ensemble_auto_mean, c = \"tab:blue\")\n",
    "plt.plot(np.linspace(0,5,251), ensemble_auto_mean-std_deviation, \"--\", c = \"tab:blue\")\n",
    "plt.plot(np.linspace(0,5,251), ensemble_auto_mean+std_deviation, \"--\", c = \"tab:blue\")\n",
    "\n",
    "plt.plot(np.linspace(0,5,251), [0]*(nlags+1), \"--\", c = \"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950c715",
   "metadata": {},
   "source": [
    "# REAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35dc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dT = 0.02\n",
    "#h = 0.001\n",
    "#L = int(dT/h)\n",
    "#t = np.arange(-108.3, -10.4, dT) \n",
    "t = np.arange(-67.73, -10.4, dT)\n",
    "t_ = [x for x in t]\n",
    "\n",
    "n = len(t) \n",
    "y = [0] * n  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da433869",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import real data:\n",
    "path = \"C:/Users/morit/documents/Masterarbeit/data/\"\n",
    "\n",
    "### NGRIP d18O 20 year resolution ###\n",
    "df = pd.read_csv(path + \"NGRIP_d18O_GICC05modelext.txt\", sep = \"\\t\", header = 66, engine = \"python\")\n",
    "df[\"Age (years b2k)\"] = -df[\"Age (years b2k)\"]/1000 # convert to kyr\n",
    "df[\"Age (years b2k)\"] = df[\"Age (years b2k)\"]-0.05 # convert b2k to BP\n",
    "df[\"Age (years b2k)\"] = df[\"Age (years b2k)\"]-0.01 # datapoint reflects midpoint of interval\n",
    "df[\"Age (years b2k)\"] = df[\"Age (years b2k)\"]*1.0063 # For consistency with WD2014 chronology and Hulu cave \n",
    "max_val, min_val = np.round(np.max(df[\"Age (years b2k)\"]),3), np.round(np.min(df[\"Age (years b2k)\"]),3)\n",
    "max_difference = np.round(df['Age (years b2k)'][df['Age (years b2k)']>t_[0]].diff().abs().max(),3)\n",
    "print(\"NGRIP1:  oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "ngrip = interp1d(df[\"Age (years b2k)\"], df[\"NGRIP d18O (permil)\"], kind=\"linear\")(t_)\n",
    "ngrip_=ngrip\n",
    "ngrip = (ngrip - np.mean(ngrip)) / np.std(ngrip)\n",
    "#plt.plot(t,ngrip, label = r\"ngrip1 d18O\")\n",
    "\n",
    "\n",
    "### NGRIP d18O 20 year resolution version 2 ###\n",
    "df = pd.read_csv(path + \"ngrip_20yr.txt\", sep = \"\\t\", header = 50, engine = \"python\")\n",
    "df_ = pd.DataFrame(df['Age'])\n",
    "df_['d18O'] = df['d18O NGRIP1'].combine_first(df['d18O NGRIP2'])\n",
    "df_['d18O'] = df_['d18O'].str.replace(',', '.')\n",
    "df_ = df_.drop(df_.index[0])\n",
    "df_['Age']=pd.to_numeric(df_['Age'])\n",
    "df_[\"Age\"] = -df_[\"Age\"]/1000 # convert to kyr\n",
    "df_[\"Age\"] = df_[\"Age\"]-0.05 # convert b2k to BP\n",
    "df_[\"Age\"] = df_[\"Age\"]-0.01\n",
    "df_[\"Age\"] = df_[\"Age\"]*1.0063 # For consistency with WD2014 chronology and Hulu cave \n",
    "max_val, min_val = np.round(np.max(df_[\"Age\"]),3), np.round(np.min(df_[\"Age\"]),3)\n",
    "max_difference = np.round(df_['Age'][df_['Age']>t_[0]].diff().abs().max(),3)\n",
    "print(\"NGRIP2:  oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "ngrip2 = interp1d(df_[\"Age\"], df_[\"d18O\"], kind=\"linear\")(t_)\n",
    "ngrip2_=ngrip2\n",
    "ngrip2 = (ngrip2 - np.mean(ngrip2)) / np.std(ngrip2)\n",
    "#plt.plot(t,ngrip2, label = r\"ngrip2 d18O\")\n",
    "\n",
    "\n",
    "### NGRIP calcium 20 year resolution ###\n",
    "df = pd.read_csv(path + \"ngrip_20yr.txt\", sep = \"\\t\", header = 50, engine = \"python\")\n",
    "df_ = pd.DataFrame(df['Age'])\n",
    "df_['Ca2+'] = df['[Ca2+] NGRIP2']\n",
    "df_['Ca2+'] = df_['Ca2+'].str.replace(',', '.')\n",
    "df_ = df_.drop(df_.index[0])\n",
    "df_['Age']=pd.to_numeric(df_['Age'])\n",
    "df_[\"Age\"] = -df_[\"Age\"]/1000\n",
    "df_[\"Age\"] = df_[\"Age\"]-0.05\n",
    "df_[\"Age\"] = df_[\"Age\"]-0.01\n",
    "df_[\"Age\"] = df_[\"Age\"]*1.0063\n",
    "df_cleaned = df_.dropna(subset=['Ca2+']) \n",
    "max_val, min_val = np.round(np.max(df_[\"Age\"]),3), np.round(np.min(df_[\"Age\"]),3)\n",
    "max_difference = np.round(df_['Age'][df_[\"Age\"]>t_[0]].diff().abs().max(),3)\n",
    "print(\"CALCIUM: oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "cal = interp1d(df_cleaned['Age'].values, df_cleaned['Ca2+'].values, kind=\"linear\")(t_)\n",
    "cal = -np.log(cal)\n",
    "cal_= cal\n",
    "cal = (cal - np.mean(cal)) / np.std(cal)\n",
    "#plt.plot(t,cal, label = r\"ngrip calcium\")\n",
    "\n",
    "### insolation north ###\n",
    "#http://vo.imcce.fr/insola/earth/online/earth/online/index.php\n",
    "df = pd.read_table(path+\"insolation_mean_mon_7_65N_100k.txt\", sep = \"     \", header = None, engine = \"python\")\n",
    "df.columns = [\"Time\", \"Insolation\"]\n",
    "df[\"Time\"] = df[\"Time\"]-0.05# + 2.000\n",
    "max_val, min_val = np.round(np.max(df[\"Time\"]),3), np.round(np.min(df[\"Time\"]),3)\n",
    "max_difference = np.round(df['Time'][df[\"Time\"]>t_[0]].diff().abs().max(),3)\n",
    "soln = interp1d(df[\"Time\"], df[\"Insolation\"], kind=\"linear\")(t_)\n",
    "soln_ = soln\n",
    "soln = (soln - np.mean(soln)) / np.std(soln)\n",
    "print(\"INSO (N): oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "#plt.plot(t,soln, label = \"sol (n)\")\n",
    "\n",
    "### insolation south ###\n",
    "df = pd.read_table(path+\"insolation_mean_mon_7_65S_100k.txt\", sep = \"     \", header = None, engine = \"python\")\n",
    "df.columns = [\"Time\", \"Insolation\"]\n",
    "df[\"Time\"] = df[\"Time\"] + 2.000\n",
    "max_val, min_val = np.round(np.max(df[\"Time\"]),3), np.round(np.min(df[\"Time\"]),3)\n",
    "max_difference = np.round(df['Time'][df[\"Time\"]>t_[0]].diff().abs().max(),3)\n",
    "sols = interp1d(df[\"Time\"], df[\"Insolation\"], kind=\"linear\")(t_)\n",
    "sols_ = sols\n",
    "sols = (sols - np.mean(sols)) / np.std(sols)\n",
    "print(\"INSO (S):  oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "#plt.plot(t,sols, label = \"sol (s)\")\n",
    "\n",
    "\n",
    "### benthic ###\n",
    "df = pd.read_csv(path  + \"Global_stack_d18O.txt\", sep = \"\\t\", header = 57, engine = \"python\")\n",
    "df[\"Age [ka BP]\"] = -df[\"Age [ka BP]\"]# + 1.950\n",
    "max_val, min_val = np.round(np.max(df[\"Age [ka BP]\"]),3), np.round(np.min(df[\"Age [ka BP]\"]),3)\n",
    "max_difference = np.round(df['Age [ka BP]'][df[\"Age [ka BP]\"]>t_[0]].diff().abs().max(),3)\n",
    "benthic = interp1d(df[\"Age [ka BP]\"], df[\"δ18O stack [‰]\"], kind=\"linear\")(t_)\n",
    "benthic_= benthic\n",
    "benthic = (benthic - np.mean(benthic)) / np.std(benthic)\n",
    "print(\"BENTHIC:  oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "#plt.plot(t, -benthic, label = \"benthic\")\n",
    "\n",
    "\n",
    "### WD d18O ###\n",
    "WD_age = pd.read_csv(path + \"WD2014_506modAKZ291b_v4.txt\", sep = \"\\t\", header = 12, engine = \"python\")\n",
    "df = pd.read_csv(path + \"WAIS_project_members_Source_Data.txt\", sep = \"\\t\", header = 44, engine = \"python\")\n",
    "df1 = df[[\"Depths Top (m)\", \"Depths Bottom (m)\"]].mean(axis=1).to_frame()\n",
    "df1.rename(columns={0: \"Depth\"}, inplace=True)\n",
    "df2 = df[[\"WD2014 Age Top (ka1950.0)\", \"WD2014 Age Bottom (ka1950.0)\"]].mean(axis=1).to_frame()\n",
    "df2.rename(columns={0: \"WD2014 Age\"}, inplace=True)\n",
    "df3 = df[\"Isotope Data d18O (per mil)\"]\n",
    "df_ = pd.concat([df1, df2, df3], axis=1)\n",
    "WD_age[\"Ice age (years BP)\"] = -WD_age[\"Ice age (years BP)\"]/1000\n",
    "df_[\"WD2014 Age\"] = interp1d(WD_age[\"Depth (m)\"], WD_age[\"Ice age (years BP)\"], kind='linear', fill_value='extrapolate')(df_[\"Depth\"])\n",
    "max_val, min_val = np.round(np.max(df_[\"WD2014 Age\"]),3), np.round(np.min(df_[\"WD2014 Age\"]),3)\n",
    "max_difference = np.round(df_[\"WD2014 Age\"][df_[\"WD2014 Age\"]>t_[0]].diff().abs().max(),3)\n",
    "print(\"WD:        oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "wd = interp1d(df_[\"WD2014 Age\"], df_[\"Isotope Data d18O (per mil)\"], kind=\"linear\")(t_)\n",
    "wd_= wd\n",
    "wd = (wd - np.mean(wd)) / np.std(wd)\n",
    "#plt.plot(t,wd, label = r\"WD\")\n",
    "\n",
    "\n",
    "### EDML d18O ###\n",
    "df = pd.read_csv(path + \"EDML.txt\", sep = \"\\t\", header = 22, engine = \"python\")\n",
    "df[\"Age [ka BP]\"] = -df[\"Age [ka BP]\"]\n",
    "max_val, min_val = np.round(np.max(df[\"Age [ka BP]\"]),3), np.round(np.min(df[\"Age [ka BP]\"]),3)\n",
    "max_difference = np.round(df['Age [ka BP]'][df[\"Age [ka BP]\"]>t_[0]].diff().abs().max(),3)\n",
    "edml = interp1d(df[\"Age [ka BP]\"], df[\"δ18O H2O [‰ SMOW]\"], kind=\"linear\")(t_)\n",
    "edml_= edml\n",
    "edml = (edml - np.mean(edml)) / np.std(edml)\n",
    "print(\"EDML:    oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "#plt.plot(t,edml, label = \"EDML\")\n",
    "\n",
    "### WD-EDC-EDML d18O stack ###\n",
    "df = pd.read_csv(path + \"antarctica_stack.txt\", sep = \"\\t\", skipinitialspace=True, header = 26, engine = \"python\")\n",
    "df[\"Age [ka BP]\"] = -df[\"Age [ka BP]\"]#+1.950\n",
    "max_val, min_val = np.round(np.max(df[\"Age [ka BP]\"]),3), np.round(np.min(df[\"Age [ka BP]\"]),3)\n",
    "max_difference = np.round(df['Age [ka BP]'][df[\"Age [ka BP]\"]>t_[0]].diff().abs().max(),3)\n",
    "print(\"ANT_ST:   oldest: \" + str(min_val) + \" | \" + \"newest: \" + str(max_val) + \" | \" + \"largest deltaT: \" + str(max_difference))\n",
    "ant_st = interp1d(df[\"Age [ka BP]\"], df[\"δ18O H2O [‰ SMOW]\"], kind=\"linear\")(t_)\n",
    "ant_st_= ant_st\n",
    "ant_st = (ant_st - np.mean(ant_st)) / np.std(ant_st)\n",
    "#plt.plot(t, ant_st, label = \"WD-EDC-EDML\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"WD2014 age (kyr BP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00f53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_red_noise(data, gauß, alpha, seed=1):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        noise = rng.normal(0, gauß, len(data))\n",
    "        #fft = np.fft.fft(noise)\n",
    "        #frequencies = np.fft.fftfreq(len(data))\n",
    "        #filter = np.power(np.abs(frequencies) + 1e-10, -alpha / 2)\n",
    "        #filtered_fft = fft * filter\n",
    "        #filtered_noise = np.real(np.fft.ifft(filtered_fft))\n",
    "        return data + noise #filtered_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6038864",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "north_true = subtract_moving(cal_, 516, \"mid\")\n",
    "south_true = subtract_moving(wd_, 516, \"mid\")\n",
    "\n",
    "num_steps = len(north_true)\n",
    "\n",
    "def correlation(params):\n",
    "    \n",
    "    S = np.zeros(num_steps)\n",
    "    N = np.zeros(num_steps)\n",
    "\n",
    "    S[0] = north_true[0]\n",
    "    N[0] = north_true[0]\n",
    "\n",
    "    for i in range(1, num_steps):\n",
    "        S[i] = S[i-1] - (dT/params[0]) * (N[i-1] + S[i-1])\n",
    "        N[i] = north_true[i]\n",
    "\n",
    "    Tsp = add_red_noise(S, params[1], 1)\n",
    "    \n",
    "    return -np.corrcoef(Tsp, south_true)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = np.arange(0.5,2, 0.005)\n",
    "corr = []\n",
    "\n",
    "lo = np.array([taus.min(), 0.2])\n",
    "up = np.array([taus.max(), 1])\n",
    "\n",
    "corr_min = minimize(correlation, [0.01, 0.001], method=\"L-BFGS-B\", bounds=list(zip(lo, up)))\n",
    "print(corr_min.x)\n",
    "\n",
    "for i in taus:\n",
    "    corr.append(-1*correlation([i, corr_min.x[1]]))\n",
    "\n",
    "plt.plot(taus, corr)\n",
    "\n",
    "plt.scatter(corr_min.x[0], -corr_min.fun)\n",
    "plt.text(corr_min.x[0], -corr_min.fun+0.002, f'({np.round(corr_min.x[0],4)}, {-np.round(corr_min.fun,4)})', ha='center', va='bottom')\n",
    "\n",
    "plt.ylim(corr[0],-corr_min.fun+0.005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04152992",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = corr_min.x[0]\n",
    "\n",
    "S = np.zeros(num_steps)\n",
    "N = np.zeros(num_steps)\n",
    "\n",
    "S[0]  = north_true[0]\n",
    "N[0]  = north_true[0]\n",
    "\n",
    "random.seed(1)\n",
    "for i in range(1, num_steps):\n",
    "    S[i] = S[i-1] - (dT/tau) * (N[i-1] + S[i-1])\n",
    "    N[i] = north_true[i]\n",
    "    \n",
    "S_ = add_red_noise(S, corr_min.x[1], 1)\n",
    "\n",
    "plt.plot(t, (south_true-np.mean(south_true))/np.std(south_true), label = 'WDC', zorder = 1)\n",
    "plt.plot(t, (S-np.mean(S))/np.std(S), label = 'Calculated from NGRIP Ca$^{2+}$', c = \"tab:orange\", zorder = 3)\n",
    "plt.plot(t, (S_-np.mean(S_))/np.std(S_), label = 'Calculated from NGRIP Ca$^{2+}$+ white noise', c = \"tab:red\", zorder = 2)\n",
    "plt.legend()\n",
    "#plt.xlim(-40,-45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca2334",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = (S-np.mean(S))/np.std(S)\n",
    "S = add_red_noise(S, 0.15, 1)\n",
    "plt.plot(S)\n",
    "plt.plot(north_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd745a7",
   "metadata": {},
   "source": [
    "### Modes: \n",
    "\n",
    "full: $[𝑑,𝑐,𝑏,𝑎,𝑁_{𝑜𝑏𝑠},𝑁_{𝑑𝑦𝑛}, benthic, sol]$\n",
    "\n",
    "no: $[𝑑,𝑐,𝑏,𝑎,𝑁_{𝑜𝑏𝑠},𝑁_{𝑑𝑦𝑛}]$\n",
    "\n",
    "benthic: $[𝑑,𝑐,𝑏,𝑎,𝑁_{𝑜𝑏𝑠},𝑁_{𝑑𝑦𝑛}, benthic]$\n",
    "\n",
    "sol: $[𝑑,𝑐,𝑏,𝑎,𝑁_{𝑜𝑏𝑠},𝑁_{𝑑𝑦𝑛}, sol]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4210295",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "data1 = north_true #cal_ \n",
    "data2 = S #wd_\n",
    "\n",
    "#subract forcing\n",
    "#data1 = list(subtract_moving(data1, 516, \"mid\"))\n",
    "#data1_return = data1\n",
    "#data2 = list(subtract_moving(data2, 516, \"mid\"))\n",
    "\n",
    "#normalize\n",
    "#data1 = data1-np.mean(data1)\n",
    "#data2 = data2-np.mean(data2)\n",
    "#data1 = (data1-np.mean(data1))/np.std(data1)\n",
    "#data2 = (data2-np.mean(data2))/np.std(data2)\n",
    "\n",
    "#cut \n",
    "#lag = (208+218)/2/1000\n",
    "#data1 = data_cut(data1,-60,-25)[1]\n",
    "#data2 = data_cut(data2,-60+lag,-25+lag)[1]\n",
    "#lag = 23\n",
    "#data1 = data_cut(data1,-60,-25)[1]\n",
    "#data2 = data_cut(data2[:-lag],-60-lag*dT,-25-lag*dT)[1]\n",
    "data1 = data_cut(data1,-60,-25)[1]\n",
    "data2 = data_cut(data2,-60,-25)[1]\n",
    "\n",
    "#plot PCA\n",
    "plt.plot(data_cut(data1,-60,-25)[0], data1, label = \"Cal\")\n",
    "plt.plot(data_cut(data1,-60,-25)[0], data2, label = \"WDC\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#ref_direction = PCA_(-data1, -data2, None, None)[1]\n",
    "\n",
    "plt.plot(data_cut(data1,-60,-25)[0], PCA_(data1, data2, 0, ref_direction)[0], label = \"PC 1\")\n",
    "plt.plot(data_cut(data1,-60,-25)[0], PCA_(data1, data2, 1, ref_direction)[0], label = \"PC 2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#biplot(data_new[:,0:2], np.transpose(pca.components_[0:2, :]), 2)\n",
    "\n",
    "combined_data = np.vstack((data1, data2)).T\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(combined_data)\n",
    "\n",
    "#Hauptkomponenten sind Vektoren im Merkmalsraum und definieren die Richtungen mit maximaler Varianz in den Daten.\n",
    "print(\"PCA1: \" + str(np.round(pca.components_[0], 2))) #1.ste Hauptkomponente\n",
    "print(\"PCA2: \" + str(np.round(pca.components_[1], 2))) #2.te Hauptkomponente\n",
    "\n",
    "#Diese Eigenschaft gibt an, wie viel von der Gesamtvarianz in den Daten von \n",
    "#jeder der ausgewählten Hauptkomponenten erklärt wird. Für den ersten Wert in \n",
    "#pca.explained_variance_ratio_ erhalten Sie den Anteil der Gesamtvarianz, der \n",
    "#von der ersten Hauptkomponente erklärt wird, und für den zweiten Wert erhalten \n",
    "#Sie den Anteil der Gesamtvarianz, der von der zweiten Hauptkomponente erklärt wird.\n",
    "print(\"Varianz:   \" + str(np.round(pca.explained_variance_ratio_*100,2)) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a09aa2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize = (5,5))\n",
    "\n",
    "ax1.scatter(data1, data2, c = \"tab:red\", s = 1, label = \"Data\")\n",
    "ax1.set_xlabel(\"Cal\", c = \"tab:red\")\n",
    "ax1.set_xlim(-3,3)\n",
    "ax1.set_ylabel(\"WDC\", c = \"tab:red\")\n",
    "ax1.set_ylim(-3,3)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.scatter(PCA_(data1, data2, 0, ref_direction)[0], PCA_(data1, data2, 1, ref_direction)[0], c = \"tab:green\", s = 1, label = \"PCA\")\n",
    "\n",
    "coeff = np.transpose(pca.components_[0:2, :])\n",
    "for i in range(2):\n",
    "    ax2.arrow(0, 0, coeff[i,0], coeff[i,1], head_width=0.2, head_length=0.2, color = 'tab:green', alpha = 1, linestyle = '-', linewidth = 1.5, overhang=0.2)\n",
    "    bbox_props = dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"tab:green\", lw=1) \n",
    "    ax2.text(coeff[i,0]* 1.4, coeff[i,1] * 1.5, \"Var\"+str(i+1), color = 'tab:green', ha = 'center', va = 'center', bbox=bbox_props)\n",
    "\n",
    "ax2.set_ylabel(\"PC 2\", c = \"tab:green\")\n",
    "ax2.set_ylim(-3,3)\n",
    "\n",
    "ax3 = ax1.twiny()\n",
    "ax3.set_xlabel('PC 1', c = \"tab:green\")\n",
    "ax3.set_xlim(-3,3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405875df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensemble = 1\n",
    "xx = np.arange(-4, 4, 0.005)\n",
    "#all_params = []\n",
    "\n",
    "for index in range(1):\n",
    "    \n",
    "    data = PCA_(data1, data2, 1, ref_direction)[0]\n",
    "  \n",
    "    ukf_1dim.mode = \"no\"\n",
    "    ukf_1dim.hem = \"north\"\n",
    "\n",
    "    bound_int = 3\n",
    "    lo = np.array([-bound_int, -bound_int, -bound_int,  0, 0.000001, 0])\n",
    "    up = np.array([bound_int,bound_int,bound_int,bound_int,bound_int,bound_int+2])\n",
    "\n",
    "    set = lhs(len(lo), samples=ensemble)                \n",
    "    set = lo + set.dot(np.diag(up - lo))\n",
    "\n",
    "    fit = minimize(ukf_1dim, [1,1,1,1,1,1], method=\"L-BFGS-B\", bounds=list(zip(lo, up)))\n",
    "    \n",
    "    if all(lo[index] < fit.x[index] < up[index] for index in range(len(fit.x))):\n",
    "        print(\"Added Parameter: \", fit.x, \"Fun: \", fit.fun)\n",
    "        #all_params.append(fit.x)\n",
    "        plt.plot(xx, [potential_fit(x) for x in xx], label=\"simulated\")\n",
    "        \n",
    "    else:\n",
    "        #print(\"NOT added Parameter: \",fit.x, \"Fun: \", fit.fun)\n",
    "        plt.plot(xx, [potential_fit(x) for x in xx], \"--\", label=\"simulated\")\n",
    "    \n",
    "    \n",
    "#average_params = np.mean(all_params, axis=0)\n",
    "\n",
    "#plt.plot(xx, [plot_potential(x,average_params) for x in xx], c = \"k\", label = \"mean\")   \n",
    "\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-2,2)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6afcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = list(fit.x)\n",
    "\n",
    "aic = 2*(len(set[0]) + fit.fun)\n",
    "bic = 2*fit.fun + len(set[0])*np.log(len(data))\n",
    "\n",
    "output.append(fit.fun)\n",
    "output.append(aic)\n",
    "output.append(bic)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PCA_(data1, data2, 0, ref_direction)[0]\n",
    "ukf_1dim.mode = \"no\"\n",
    "ukf_1dim.hem = \"north\"\n",
    "fit_x = fit.x\n",
    "Hfun = ndt.Hessian(ukf_1dim, full_output=True)\n",
    "hessian_ndt  = Hfun(fit_x)\n",
    "dx = list(np.sqrt(np.diag(np.linalg.inv(hessian_ndt[0]))))\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90879365",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_std(dx, 60)\n",
    "#plot_std_(dx)\n",
    "plt.plot(xx, plot_potential(xx, fit.x))\n",
    "plt.ylim(-2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(-4, 4, 0.005)\n",
    "\n",
    "#plot_std(dx, 80)\n",
    "plt.plot(xx, [potential_fit(x) for x in xx])\n",
    "\n",
    "plt.xlim(-1.5,1.5)\n",
    "plt.ylim(-3,3)\n",
    "\n",
    "plt.title(\"PCA from Antarctica and Greenland after removing large \\n scale trend by the first two SSA. Data cut to -70 to -20 kyr BP\")\n",
    "#plt.savefig('C:/Users/morit/Documents/Masterarbeit/output_M/PCA.jpg', format='jpg', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "plot_std(dx, 70)\n",
    "\n",
    "plt.plot(xx, [potential_fit(x) for x in xx])\n",
    "plt.ylim(-1,1)\n",
    "plt.xlim(xx[0],xx[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15016003",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "p = fit.x\n",
    "w = np.zeros(n)\n",
    "z = data[0]\n",
    "n = len(data)\n",
    "\n",
    "w[0] = z + random.gauss(0, p[4]) \n",
    "for k in range(1, n):\n",
    "    for l in range(int(L)):\n",
    "        z = z - h*(4 * p[3]*z**3 + 3*p[2]*z**2 + 2*p[1]*z + p[0]) + random.gauss(0, p[5] * np.sqrt(h))\n",
    "    w[k] = z + random.gauss(0, p[4]) # observation at every L(=20) years\n",
    "\n",
    "### plotting two time series\n",
    "plt.plot(data, label=\"given\")\n",
    "plt.plot(w, label=\"simulated\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877db539",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_auto = []\n",
    "for i in range(100):\n",
    "    w[0] = z + random.gauss(0, p[4]) \n",
    "    for k in range(1, n):\n",
    "        for l in range(int(L)):\n",
    "            z = z - h*(4 * p[3]*z**3 + 3*p[2]*z**2 + 2*p[1]*z + p[0]) + random.gauss(0, p[5] * np.sqrt(h))\n",
    "        w[k] = z + random.gauss(0, p[4]) \n",
    "    ensemble_auto.append(stattools.acf(w, nlags=nlags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8e6129",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = y\n",
    "nlags = int(5/0.02)\n",
    "\n",
    "autocorr = stattools.acf(x, nlags=nlags)\n",
    "plt.plot(np.linspace(0,5,251), autocorr, c = \"tab:green\")\n",
    "\n",
    "autocorr = stattools.acf(w, nlags=nlags)\n",
    "ensemble_auto_mean = np.mean(ensemble_auto, axis=0)\n",
    "\n",
    "std_deviation = np.std(ensemble_auto, axis=0)\n",
    "plt.plot(np.linspace(0,5,251), ensemble_auto_mean, c = \"tab:blue\")\n",
    "plt.plot(np.linspace(0,5,251), ensemble_auto_mean-std_deviation, \"--\", c = \"tab:blue\")\n",
    "plt.plot(np.linspace(0,5,251), ensemble_auto_mean+std_deviation, \"--\", c = \"tab:blue\")\n",
    "\n",
    "plt.plot(np.linspace(0,5,251), [0]*(nlags+1), \"--\", c = \"k\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
